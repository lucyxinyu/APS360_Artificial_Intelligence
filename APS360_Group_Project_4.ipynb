{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS360_Group_Project-4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VMdv4x1X9zuo",
        "ejE92EGm-L3p",
        "Yc0adivD_hOQ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucyxinyu/APS360_Artificial_Intelligence/blob/master/APS360_Group_Project_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3-mnp_10zBk"
      },
      "source": [
        "#APS360 Final Project - Team 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmTpiBA1JRXu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdv4x1X9zuo"
      },
      "source": [
        "##Spectrogram Test\n",
        "The following code plots a spectrogram of a song you want from YouTube."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejE92EGm-L3p"
      },
      "source": [
        "####YouTube download code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBRj0u9A92iT"
      },
      "source": [
        "#Install youtube-dl library to download stuff from YouTube\n",
        "!pip install youtube-dl #only need to run this once"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja-wzIur-KrX"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "from IPython.display import Audio\n",
        "import youtube_dl\n",
        "\n",
        "def download_music_youtube(link, codec='mp3'):\n",
        "    if link != \"\":\n",
        "        if \"https://www.youtube.com/\" not in link:\n",
        "            return \"\"\n",
        "    else:\n",
        "        return \"\"\n",
        "    \n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([link])\n",
        "        info_dict = ydl.extract_info(link, download=False)\n",
        "        video_url = info_dict.get(\"url\", None)\n",
        "        video_id = info_dict.get(\"id\", None)\n",
        "        video_title = info_dict.get('title', None)\n",
        "    filename = video_title + \"-\" + video_id + \".\" + codec\n",
        "    return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcmyKb19-azH"
      },
      "source": [
        "link = \"https://www.youtube.com/watch?v=9E6b3swbnWg\" # <<------- ENTER THE LINK OF THE YOUTUBE VIDEO YOU WANT TO DOWNLOAD\n",
        "music_file = download_music_youtube(link)\n",
        "Audio(music_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc0adivD_hOQ"
      },
      "source": [
        "####Spectrogram code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-aZ1rQ_j1I"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "def get_spec_log(filename, offset=0, duration=None, sr=None, n_fft=2048, hop_length=512):\n",
        "    \"\"\"\n",
        "    Generates a numpy array that you can use to plot a spectrogram (with logarithmic scale)\n",
        "    INPUTS:\n",
        "        ::string:: filename     #name of the file\n",
        "        ::float::  offset       #starting time of the song in seconds\n",
        "        ::float::  duration     #how much of the song you want to load in seconds\n",
        "        ::int::    sr           #sampling rate; select None for original sampling rate of song\n",
        "        ::int::    n_fft        #size of the FFT, which will also be used as the window length\n",
        "        ::int::    hop_length   #step or stride between windows. If the step is smaller than the window length, the windows will overlap\n",
        "    OUTPUTS:\n",
        "        ::np.ndarray:: y        #data which represents the song\n",
        "        ::int::        sr       #sampling rate\n",
        "        ::np.ndarray:: D_log    #use this to plot spectrogram\n",
        "    \"\"\"\n",
        "    if os.path.isfile(filename) == False:\n",
        "        return []\n",
        "    y, sr = librosa.load(filename, offset=offset, duration=duration, sr=sr)\n",
        "    D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=n_fft))\n",
        "    D_log = librosa.amplitude_to_db(D,ref=np.max)\n",
        "    return y, sr, D_log\n",
        "\n",
        "def plot_spec(D_log, sr, title=\"Power spectrogram\"):\n",
        "    librosa.display.specshow(D_log,y_axis='log', x_axis='time', sr=sr)\n",
        "    plt.title(title)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58_J9Brk_uwN"
      },
      "source": [
        "y, sr, D_log = get_spec_log(music_file, offset=0, duration=10, n_fft=10000)\n",
        "plot_spec(D_log, sr, title=music_file)\n",
        "librosa.display.waveplot(y, sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snW9d7k18YTO"
      },
      "source": [
        "##Data Splitting and Visualization\n",
        "\n",
        "The following code is used to import the MusicNet data, process the labels into interval trees, and for obtaining visualizations of the data and labels, through a spectrogram and transcription plot respectively. In order for this to work, the dataset must be uploaded to the current working directory, with the right structure (with the right subfolders train_data, train_labels, etc.).<br>\n",
        "\n",
        "Note that the working directory must be structured like this:<br>\n",
        "```\n",
        "<current working directory>\n",
        "├── musicnet\n",
        "|   ├── test_data\n",
        "|   ├── test_images\n",
        "|   ├── test_labels\n",
        "|   ├── train_data\n",
        "|   ├── train_images\n",
        "|   └── train_labels\n",
        "└── <this notebook>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIHWnJd98cak"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "from intervaltree import IntervalTree\n",
        "import librosa\n",
        "import librosa.display\n",
        "import cv2 as cv\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7DkxUs-N1s8"
      },
      "source": [
        "##################################################################\n",
        "#GLOBAL VARIABLES\n",
        "\n",
        "root = os.getcwd() #working directory\n",
        "\n",
        "#location of the data files\n",
        "data_set_dir = os.path.join(root, \"musicnet\")\n",
        "train_data_dir = os.path.join(data_set_dir, \"train_data\")\n",
        "train_labels_dir = os.path.join(data_set_dir, \"train_labels\")\n",
        "test_data_dir = os.path.join(data_set_dir, \"test_data\")\n",
        "test_labels_dir = os.path.join(data_set_dir, \"test_labels\")\n",
        "\n",
        "train_images_dir = os.path.join(data_set_dir, \"train_images\")\n",
        "test_images_dir = os.path.join(data_set_dir, \"test_images\")\n",
        "if os.path.isdir(train_images_dir) == False:\n",
        "    os.mkdir(train_images_dir)\n",
        "if os.path.isdir(test_images_dir) == False:\n",
        "    os.mkdir(test_images_dir)\n",
        "\n",
        "filenums = []\n",
        "\n",
        "fs = 44100      # samples/second\n",
        "\n",
        "##################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv7T_Svx8zok"
      },
      "source": [
        "#FUNCTIONS TO SET UP THE DATASET\n",
        "\n",
        "def getfilenums(path, ret_var_type=\"int\"):\n",
        "    \"\"\"\n",
        "    Returns list of base filenames (without extension) in path, with each entry having type ret_var_type\n",
        "    \"\"\"\n",
        "    a = []\n",
        "    if os.path.isdir(path) == False:\n",
        "        return a\n",
        "    if ret_var_type == \"int\":\n",
        "        for entry in os.scandir(path):\n",
        "            if entry.path.endswith(\".csv\") and entry.is_file():\n",
        "                a += [int(os.path.basename(entry)[0:-4])]\n",
        "    elif ret_var_type == \"str\":\n",
        "        for entry in os.scandir(path):\n",
        "            if entry.path.endswith(\".csv\") and entry.is_file():\n",
        "                a += [os.path.basename(entry)[0:-4]]\n",
        "    return a\n",
        "\n",
        "###################################################################################################################\n",
        "def process_labels(path):\n",
        "    \"\"\"\n",
        "    Takes in csv files in the directory, path, and returns a dict with entries that are intervaltree.intervaltree.IntervalTree\n",
        "    \"\"\"\n",
        "    #This function was taken and modified from https://github.com/jthickstun/pytorch_musicnet/blob/master/musicnet.py\n",
        "    trees = dict()\n",
        "    for item in os.listdir(os.path.join(data_set_dir,path)):\n",
        "        if not item.endswith('.csv'): continue\n",
        "        uid = int(item[:-4])\n",
        "        tree = IntervalTree()\n",
        "        with open(os.path.join(data_set_dir,path,item), 'r') as f:\n",
        "            reader = csv.DictReader(f, delimiter=',')\n",
        "            for label in reader:\n",
        "                start_time = int(label['start_time'])\n",
        "                end_time = int(label['end_time'])\n",
        "                instrument = int(label['instrument'])\n",
        "                note = int(label['note'])\n",
        "                start_beat = float(label['start_beat'])\n",
        "                end_beat = float(label['end_beat'])\n",
        "                note_value = label['note_value']\n",
        "                tree[start_time:end_time] = (instrument,note,start_beat,end_beat,note_value)\n",
        "        trees[uid] = tree\n",
        "    return trees\n",
        "\n",
        "###################################################################################################################\n",
        "\n",
        "def get_music_filenames(path):\n",
        "    d = dict()\n",
        "    for entry in os.scandir(path):\n",
        "        if entry.path.endswith(\".wav\") and entry.is_file():\n",
        "            d[int(os.path.basename(entry)[0:-4])] = entry.path\n",
        "    return d\n",
        "\n",
        "def get_dataset(dataset_type='train'):\n",
        "    if dataset_type == 'train':\n",
        "        label_data = process_labels(train_labels_dir)\n",
        "        music_data = get_music_filenames(train_data_dir)\n",
        "    elif dataset_type == 'test':\n",
        "        label_data = process_labels(test_labels_dir)\n",
        "        music_data = get_music_filenames(test_data_dir)\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "    #merge the two dictionaries\n",
        "    ds = [music_data, label_data]\n",
        "    dataset = dict()\n",
        "    for k in music_data.keys():\n",
        "        dataset[k] = tuple(dataset[k] for dataset in ds)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT-sJhWM87dK"
      },
      "source": [
        "#FUNCTION FOR FURTHER DEALING WITH THE LABELS (TRANSCRIPTION)\n",
        "\n",
        "def plot_MIDI(intervaltree_obj, start_time=0, end_time=30):\n",
        "    \"\"\"\n",
        "    Plots the transcription in MIDI format of an intervaltree.intervaltree.IntervalTree object\n",
        "    \"\"\"\n",
        "    stride = 512                         # 512 samples between windows\n",
        "    wps = fs/float(stride)               # ~86 windows/second; fs is global variable defined previously\n",
        "    \n",
        "    Yvec = np.zeros((int(wps*(end_time-start_time)),128))   # 128 distinct note labels\n",
        "    \n",
        "    #See http://www.ccarh.org/courses/253/handout/gminstruments/ for MIDI codes\n",
        "    MIDI_codes = {\n",
        "        1: 'Piano',\n",
        "        41: 'Violin',\n",
        "        42: 'Viola',\n",
        "        43: 'Cello',\n",
        "        72: 'Clarinet',\n",
        "        71: 'Bassoon',\n",
        "        61: 'French Horn',\n",
        "        69: 'Oboe',\n",
        "        74: 'Flute',\n",
        "        7: 'Harpsichord',\n",
        "        44: 'Contrabass'\n",
        "    }\n",
        "    #Note that there are only 11 instruments in the dataset\n",
        "    MIDI_colour_codes = {\n",
        "        1: 1,\n",
        "        41: 2,\n",
        "        42: 3,\n",
        "        43: 4,\n",
        "        72: 5,\n",
        "        71: 6,\n",
        "        61: 7,\n",
        "        69: 8,\n",
        "        74: 9,\n",
        "        7: 10,\n",
        "        44: 11\n",
        "    }\n",
        "    #To replace the ticks on the colorbar\n",
        "    instrum_ticks = ['Piano','Violin','Viola','Cello','Clarinet','Bassoon','French Horn','Oboe','Flute','Harpsichord','Contrabass']\n",
        "\n",
        "    for window in range(Yvec.shape[0]): #iterate over Yvec\n",
        "        for obj in intervaltree_obj[window*stride + start_time*fs]: #iterate over Interval objects within window\n",
        "            Yvec[window,obj.data[1]] = MIDI_colour_codes[obj.data[0]]\n",
        "\n",
        "    fig = plt.figure(figsize=(20,5))\n",
        "    colormap = plt.imshow(Yvec.T,aspect='auto',cmap='nipy_spectral')\n",
        "    plt.gca().invert_yaxis()\n",
        "    fig.axes[0].set_xlabel('window')\n",
        "    fig.axes[0].set_ylabel('note (MIDI code)')\n",
        "\n",
        "    cbar = plt.colorbar(colormap, orientation='vertical')\n",
        "    cbar.set_ticks(range(1,12,1))\n",
        "    cbar.set_ticklabels(instrum_ticks)\n",
        "    plt.show()\n",
        "\n",
        "############################################################################################################################\n",
        "    \n",
        "def get_label_3D_array(intervaltree_obj, width, start_time=0, end_time=30, plot=True):\n",
        "    \"\"\"\n",
        "    Returns an array with shape 128 x width x 11 corresponding to the transcription of some song,\n",
        "    between some start_time and end_time, represented by an intervaltree_obj\n",
        "    \"\"\"\n",
        "    #stride = 512                         # 512 samples between windows\n",
        "    #wps = fs/float(stride)               # ~86 windows/second; fs is global variable defined previously\n",
        "    #Yvec = np.zeros((int(wps*(end_time-start_time)),128))   # 128 distinct note labels\n",
        "    \n",
        "    Yvec = np.zeros((128,width,11))   # 128 distinct note labels, 11 instruments\n",
        "    wps = int(width / (end_time-start_time))\n",
        "    stride = int(fs/wps)\n",
        "    #See http://www.ccarh.org/courses/253/handout/gminstruments/ for MIDI codes\n",
        "    MIDI_codes = {\n",
        "        1: 'Piano',\n",
        "        41: 'Violin',\n",
        "        42: 'Viola',\n",
        "        43: 'Cello',\n",
        "        72: 'Clarinet',\n",
        "        71: 'Bassoon',\n",
        "        61: 'French Horn',\n",
        "        69: 'Oboe',\n",
        "        74: 'Flute',\n",
        "        7: 'Harpsichord',\n",
        "        44: 'Contrabass'\n",
        "    }\n",
        "    #Note that there are only 11 instruments in the dataset\n",
        "    MIDI_colour_codes = {\n",
        "        1: 1,\n",
        "        41: 2,\n",
        "        42: 3,\n",
        "        43: 4,\n",
        "        72: 5,\n",
        "        71: 6,\n",
        "        61: 7,\n",
        "        69: 8,\n",
        "        74: 9,\n",
        "        7: 10,\n",
        "        44: 11\n",
        "    }\n",
        "    MIDI_colour_codes_0 = dict()\n",
        "    for key in MIDI_colour_codes:\n",
        "        MIDI_colour_codes_0[key] = MIDI_colour_codes[key] - 1\n",
        "    #MIDI_inverted_0 = {v: k for k, v in MIDI_colour_codes_0.items()}\n",
        "\n",
        "\n",
        "    for window in range(Yvec.shape[1]): #iterate over Yvec\n",
        "        for obj in intervaltree_obj[window*stride + start_time*fs]: #iterate over Interval objects within window\n",
        "            Yvec[obj.data[1],window,MIDI_colour_codes_0[obj.data[0]]] = 1\n",
        "\n",
        "    if plot == True: \n",
        "        plot_label_3D_array(Yvec)\n",
        "    return Yvec\n",
        "\n",
        "def plot_label_3D_array(Yvec):\n",
        "    \"\"\"\n",
        "    Call this function to visualize the transcription given some Yvec\n",
        "    \"\"\"\n",
        "    #To replace the ticks on the colorbar\n",
        "    instrum_ticks = ['Piano','Violin','Viola','Cello','Clarinet','Bassoon','French Horn','Oboe','Flute','Harpsichord','Contrabass']\n",
        "    \n",
        "    Y_reg = np.zeros((128,Yvec.shape[1]))\n",
        "    \n",
        "    #convert the \"one-hot encoding\" array Yvec to regular array\n",
        "    for i in range(0,Y_reg.shape[0],1):\n",
        "        for j in range(0,Y_reg.shape[1],1):\n",
        "            instr = -1\n",
        "            for k in range(0,Yvec.shape[2],1):\n",
        "                if Yvec[i,j,k] != 0:\n",
        "                    instr = k\n",
        "                    break\n",
        "            if instr != -1:\n",
        "                Y_reg[i,j] = instr + 1\n",
        "\n",
        "    fig = plt.figure(figsize=(20,5))\n",
        "    colormap = plt.imshow(Y_reg,aspect='auto',cmap='nipy_spectral')\n",
        "    plt.gca().invert_yaxis()\n",
        "    fig.axes[0].set_xlabel('window')\n",
        "    fig.axes[0].set_ylabel('note (MIDI code)')\n",
        "\n",
        "    cbar = plt.colorbar(colormap, orientation='vertical')\n",
        "    cbar.set_ticks(range(1,12,1))\n",
        "    cbar.set_ticklabels(instrum_ticks)\n",
        "    plt.show()\n",
        "    \n",
        "############################################################################################################################\n",
        "# Functions to save the labels of each song as a json file\n",
        "\n",
        "class NumpyArrayEncoder(json.JSONEncoder):\n",
        "    #see https://pynative.com/python-serialize-numpy-ndarray-into-json/\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n",
        "    \n",
        "def DictWithNumpyToJson(d, save_directory, base_filename):\n",
        "    if type(save_directory) != str or type(base_filename) != str:\n",
        "        print(\"ERROR: Invalid save_directory or filename\")\n",
        "        return False\n",
        "    if os.path.isdir(save_directory) == False:\n",
        "        print(\"ERROR: Invalid Directory\")\n",
        "        return False\n",
        "    \n",
        "    save_filename = os.path.join(save_directory, base_filename) + \".json\"\n",
        "    if os.path.exists(save_filename) == True:\n",
        "        os.remove(save_filename)\n",
        "    \n",
        "    with open(save_filename, \"w\") as write_file:\n",
        "        json.dump(d, write_file, cls=NumpyArrayEncoder)\n",
        "    return True\n",
        "    \n",
        "def JsonToDictWithNumpy(filename):\n",
        "    if os.path.isfile(filename) == False:\n",
        "        print(\"ERROR: File could not be found\")\n",
        "        return None\n",
        "    if filename.endswith(\".json\") == False:\n",
        "        print(\"ERROR: File not json\")\n",
        "        return None\n",
        "    \n",
        "    with open(filename, \"r\") as read_file:\n",
        "        opened_dict = json.load(read_file)\n",
        "    \n",
        "    return_dict = dict()\n",
        "    for key in opened_dict:\n",
        "        return_dict[key] = np.asarray(opened_dict[key])\n",
        "    \n",
        "    return return_dict\n",
        "\n",
        "def save_json_label_for_song(song_filename, save_directory, intervaltree_obj, width, interval, sr=None):\n",
        "    \"\"\"\n",
        "    Saves the label of one song, which is a json file consisting of numpy arrays produced by the get_label_3D_array function\n",
        "    INPUTS:\n",
        "        ::string:: song_filename                                    #full path of the song\n",
        "        ::string:: save_directory                                   #directory to save the json file\n",
        "        ::intervaltree.intervaltree.IntervalTree:: intervaltree_obj #interval tree object\n",
        "        ::int:: width                                               #the width in pixels of the desired transcription label (corresponds to width of spectrogram)\n",
        "        ::float:: interval                                          #the duration of each desired transcription label (corresponds to that of the spectrogram)\n",
        "        ::int:: sr                                                  #sampling rate of the song\n",
        "    \"\"\"\n",
        "    temp_y, temp_sr = librosa.load(song_filename, sr=sr)\n",
        "    song_length = librosa.get_duration(y=temp_y, sr=temp_sr) #length of the song in seconds\n",
        "    number_of_arrays = int(np.floor(song_length / interval)) #number of arrays to create\n",
        "    \n",
        "    store = dict()\n",
        "    bn = os.path.basename(song_filename)[0:-4]\n",
        "    current = ''\n",
        "    \n",
        "    for i in range(0, number_of_arrays, 1):\n",
        "        current = bn + '_' + str(i*interval).zfill(4) + '_' + str((i+1)*interval).zfill(4) \n",
        "        label_3D_array = get_label_3D_array(intervaltree_obj, width, start_time=i*interval, end_time=(i+1)*interval, plot=False)\n",
        "        store[current] = label_3D_array\n",
        "    \n",
        "    DictWithNumpyToJson(store, save_directory, bn)\n",
        "    return True\n",
        "\n",
        "############################################################################################################################\n",
        "\n",
        "def get_label_dict_for_song(song_filename, intervaltree_obj, width, interval, sr=None):\n",
        "    \"\"\"\n",
        "    Obtains the labels of one song, which is a dictionary consisting of numpy arrays produced by the get_label_3D_array function\n",
        "    INPUTS:\n",
        "        ::string:: song_filename                                    #full path of the song\n",
        "        ::intervaltree.intervaltree.IntervalTree:: intervaltree_obj #interval tree object\n",
        "        ::int:: width                                               #the width in pixels of the desired transcription label (corresponds to width of spectrogram)\n",
        "        ::float:: interval                                          #the duration of each desired transcription label (corresponds to that of the spectrogram)\n",
        "        ::int:: sr                                                  #sampling rate of the song\n",
        "    \"\"\"\n",
        "    temp_y, temp_sr = librosa.load(song_filename, sr=sr)\n",
        "    song_length = librosa.get_duration(y=temp_y, sr=temp_sr) #length of the song in seconds\n",
        "    number_of_arrays = int(np.floor(song_length / interval)) #number of arrays to create\n",
        "    \n",
        "    store = dict() #this is the dictionary that will be returned\n",
        "    bn = os.path.basename(song_filename)[0:-4]\n",
        "    current = ''\n",
        "    \n",
        "    for i in range(0, number_of_arrays, 1):\n",
        "        current = bn + '_' + str(i*interval).zfill(4) + '_' + str((i+1)*interval).zfill(4) \n",
        "        label_3D_array = get_label_3D_array(intervaltree_obj, width, start_time=i*interval, end_time=(i+1)*interval, plot=False)\n",
        "        store[current] = label_3D_array\n",
        "    \n",
        "    return store\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4vyGkEs89tc"
      },
      "source": [
        "#FUNCTIONS FOR OBTAINING THE SPECTROGRAM\n",
        "\n",
        "def get_spec_log(filename, offset=0, duration=None, sr=None, n_fft=2048, hop_length=512):\n",
        "    \"\"\"\n",
        "    Generates a numpy array that you can use to plot a spectrogram (with logarithmic scale)\n",
        "    INPUTS:\n",
        "        ::string:: filename     #name of the file\n",
        "        ::float::  offset       #starting time of the song in seconds\n",
        "        ::float::  duration     #how much of the song you want to load in seconds\n",
        "        ::int::    sr           #sampling rate; select None for original sampling rate of song\n",
        "        ::int::    n_fft        #size of the FFT, which will also be used as the window length\n",
        "        ::int::    hop_length   #step or stride between windows. If the step is smaller than the window length, the windows will overlap\n",
        "    OUTPUTS:\n",
        "        ::np.ndarray:: y        #data which represents the song\n",
        "        ::int::        sr       #sampling rate\n",
        "        ::np.ndarray:: D_log    #use this to plot spectrogram\n",
        "    \"\"\"\n",
        "    if os.path.isfile(filename) == False:\n",
        "        return []\n",
        "    y, sr = librosa.load(filename, offset=offset, duration=duration, sr=sr)\n",
        "    D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=n_fft))\n",
        "    D_log = librosa.amplitude_to_db(D,ref=np.max)\n",
        "    return y, sr, D_log\n",
        "\n",
        "def plot_spec(D_log, sr, title=\"Power spectrogram\"):\n",
        "    plt.figure(figsize=(18,5)) #include this line BEFORE the next one! https://gist.github.com/mailletf/3484932dd29d62b36092\n",
        "    axes = librosa.display.specshow(D_log,y_axis='log', x_axis='time', sr=sr)\n",
        "    plt.title(title)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.show()\n",
        "\n",
        "def show_spectrogram(filename, offset=0, duration=None, sr=None, n_fft=2048, hop_length=512, title=\"Power spectrogram\"):\n",
        "    \"\"\"\n",
        "    Call this function if you only want to see the spectrogram for some filename\n",
        "    \"\"\"\n",
        "    y, sr, D_log = get_spec_log(filename, offset=offset, duration=duration, n_fft=n_fft)\n",
        "    plot_spec(D_log, sr, title=title)\n",
        "\n",
        "############################################################################################################################\n",
        "    \n",
        "def save_spec_raw(D_log, sr, filename='temp.jpg'):\n",
        "    \"\"\"\n",
        "    Saves a raw spectrogram, without axes, titles, or labels\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(18,6), dpi=100)\n",
        "    #axes = librosa.display.specshow(D_log, y_axis='log',x_axis='time', sr=sr) #colour spectrogram\n",
        "    axes = librosa.display.specshow(D_log, cmap='gray', y_axis='log',x_axis='time', sr=sr) #grayscale spectrogram\n",
        "    plt.axis('off') #do not show axis labels\n",
        "    #plt.show()\n",
        "    plt.savefig(filename,bbox_inches='tight',pad_inches = 0) #remember to remove white space\n",
        "    plt.close() #close the plot, save memory\n",
        "    \n",
        "def get_merged_spec(filename, save_directory, offset=0, duration=None, sr=None, n_fft=2048, hop_length=512, length=20):\n",
        "    \"\"\"\n",
        "    Get the merged spectrogram of a song with filename <filename>, consisting of individual spectrograms with length <length>\n",
        "    Final spectrogram is automatically saved in in <save_directory>\n",
        "    offset, duration, sr, n_fft, hop_length are as defined in the get_spec_log function\n",
        "    \"\"\"\n",
        "    if duration == None:\n",
        "        return False\n",
        "    #check if filename exists and is a wav file\n",
        "    if os.path.exists(filename) and filename.endswith('.wav'):\n",
        "        bn = os.path.basename(filename)[0:-4]\n",
        "    else:\n",
        "        return False\n",
        "    #check if the save directory exists\n",
        "    if os.path.isdir(save_directory) == False:\n",
        "        return False\n",
        "    \n",
        "    img_filenames = [] #list that stores the filenames of the spectrogram images (each having a length defined by the variable, length)\n",
        "    num_plots = int(np.floor(duration / length)) #number of spectrograms to make\n",
        "    for i in range(0,num_plots,1):\n",
        "        current_filename = os.path.join(save_directory, bn + '_temp_' + str(offset + length*i).zfill(4) + \"_\" + str(offset + length*(i+1)).zfill(4) + '.jpg')\n",
        "        img_filenames += [current_filename]\n",
        "        y, sr, D_log = get_spec_log(filename, offset=offset+i*length, duration=length, n_fft=5000)\n",
        "        save_spec_raw(D_log, sr, filename=current_filename) #save the spectrogram\n",
        "    \n",
        "    img_array = [] #list that stores the images\n",
        "    #load images\n",
        "    for i in range(0,len(img_filenames),1):\n",
        "        #img_array += [cv.imread(img_filenames[i],-1)]\n",
        "        img_array += [cv.imread(img_filenames[i],cv.IMREAD_GRAYSCALE)]\n",
        "    \n",
        "    #merge images\n",
        "    #final_img = np.zeros((img_array[0].shape[0],img_array[0].shape[1] * len(img_filenames),3), np.uint8)\n",
        "    final_img = np.zeros((img_array[0].shape[0],img_array[0].shape[1] * len(img_filenames)), np.uint8)\n",
        "    for i in range(0,len(img_filenames),1):\n",
        "        #final_img[0:img_array[i].shape[0], (i*img_array[i].shape[1]):((i+1)*img_array[i].shape[1]), :] = img_array[i]\n",
        "        final_img[0:img_array[i].shape[0], (i*img_array[i].shape[1]):((i+1)*img_array[i].shape[1])] = img_array[i]\n",
        "    \n",
        "    #plt.imshow(final_img[:,:,::-1]) #only need to change to RGB for plotting\n",
        "    cv.imwrite(os.path.join(save_directory, bn + \"_\" + str(offset).zfill(4) + \"_\" + str(offset + length*num_plots).zfill(4) + \".jpg\"),final_img)\n",
        "    return True\n",
        "\n",
        "def get_merged_specs_of_song(filename, save_directory, sr=None, n_fft=2048, hop_length=512, length=20, interval=60):\n",
        "    \"\"\"\n",
        "    Get the merged spectrograms of an entire song; each spectrogram having a length of <interval>\n",
        "    Each merged spectrogram is made up of individual spectrograms each having a length of <length>\n",
        "    \"\"\"\n",
        "    if os.path.exists(filename) == False:\n",
        "        return False\n",
        "    if filename.endswith('.wav') == False:\n",
        "        return False\n",
        "    if os.path.isdir(save_directory) == False:\n",
        "        return False\n",
        "    \n",
        "    temp_y, temp_sr = librosa.load(filename, sr=sr)\n",
        "    song_length = librosa.get_duration(y=temp_y, sr=temp_sr) #length of the song in seconds\n",
        "    number_of_specs = int(np.floor(song_length / interval)) #number of merged spectrograms to create\n",
        "    \n",
        "    for i in range(0,number_of_specs,1):\n",
        "        get_merged_spec(filename, save_directory, offset=i*interval, duration=interval, sr=sr, n_fft=n_fft, hop_length=hop_length, length=length)\n",
        "    \n",
        "    return True\n",
        "\n",
        "def process_data_to_spec(filename, save_directory, sr=None, n_fft=2048, hop_length=512, length=20, interval=60, cleanup=True):\n",
        "    \"\"\"\n",
        "    Get the merged spectrograms of an entire song with filename <filename>; each spectrogram having a length of <interval>\n",
        "    Each merged spectrogram is made up of individual spectrograms each having a length of <length>\n",
        "    The spectrograms are saved in the directory <save_directory>, and temporary files are deleted if cleanup==True\n",
        "    \"\"\"\n",
        "    if os.path.isdir(save_directory) == False:\n",
        "        print(\"ERROR: Invalid Directory\")\n",
        "        return False\n",
        "    get_merged_specs_of_song(filename, save_directory, sr=sr, n_fft=n_fft, hop_length=hop_length, length=length, interval=interval)\n",
        "    if cleanup == True:\n",
        "        for entry in os.scandir(save_directory):\n",
        "            if entry.path.endswith(\".jpg\") and entry.is_file():\n",
        "                if '_temp_' in os.path.basename(entry)[0:-4]:\n",
        "                    os.remove(entry.path) #remove the temp files\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNALnicM9Bqd"
      },
      "source": [
        "class MusicNetDataset(torch.utils.data.Dataset):\n",
        "    #Note that this class inherits from torch.utils.data.Dataset\n",
        "    def __init__(self, dataset_type='train', create_spec=True, create_labels=False, no_saved_labels=True):\n",
        "        \"\"\"\n",
        "        INPUTS:\n",
        "            ::string:: dataset_type           #the dataset type, either 'train', 'test', or 'val' (not yet supported)\n",
        "            ::boolean:: create_spec           #if True, then merged spectrographs will be created and saved; old spectrographs will be overridden\n",
        "            ::boolean:: create_labels         #if True and if no_saved_labels==False, then will create json files of the labels; old json files will be overridden\n",
        "            ::boolean:: no_saved_labels       #if True, then will not use or create json files. Labels are loaded and processed from the csv files directly and are not saved\n",
        "        \"\"\"\n",
        "        self.filenums = [] #list of base filenames (without extension); default type of each element is int\n",
        "        self.data_dir_name = None\n",
        "        self.labels_dir_name = None\n",
        "        self.images_dir_name = None\n",
        "        \n",
        "        #Dictionary that stores the initial dataset; each entry contains the full song path and an intervaltree.intervaltree.IntervalTree object for that song\n",
        "        self.unprocessed_dataset = dict()\n",
        "        \n",
        "        #list of processed data, every entry is a dictionary\n",
        "        self.processed_data_list = []\n",
        "        #Pandas DataFrame that stores the processed dataset\n",
        "        self.processed_dataset = None\n",
        "        \n",
        "        self.interval = 10 #interval of each spectrogram, in seconds\n",
        "        self.spectrogram_width = 0 #width of each spectrogram, in pixels\n",
        "        \n",
        "        #check if we want train, test, or val\n",
        "        if dataset_type == 'train':\n",
        "            self.filenums = getfilenums(train_labels_dir)\n",
        "            self.data_dir_name = train_data_dir\n",
        "            self.labels_dir_name = train_labels_dir\n",
        "            self.images_dir_name = train_images_dir\n",
        "        elif dataset_type == 'test':\n",
        "            self.filenums = getfilenums(test_labels_dir)\n",
        "            self.data_dir_name = test_data_dir\n",
        "            self.labels_dir_name = test_labels_dir\n",
        "            self.images_dir_name = test_images_dir\n",
        "        else:\n",
        "            raise NameError(\"Invalid dataset_type\")\n",
        "        \n",
        "        #Check if self.images_dir_name is empty\n",
        "        if create_spec == False and len(os.listdir(self.images_dir_name)) == 0:\n",
        "            raise Exception(\"ERROR: There are no files in the images directory. Set create_spec=True to create spectrograms.\")\n",
        "        \n",
        "        self.unprocessed_dataset = get_dataset(dataset_type=dataset_type) #load the dictionary\n",
        "        \n",
        "        ########################################################################################################################\n",
        "        #create and save new merged spectrograms, overriding the old ones in self.images_dir_name\n",
        "        if create_spec == True:\n",
        "            self.music_filenames = get_music_filenames(self.data_dir_name)\n",
        "            for i in range(0,len(self.filenums),1):\n",
        "                process_data_to_spec(self.unprocessed_dataset[self.filenums[i]][0], self.images_dir_name, n_fft=5000, length=10, interval=self.interval, cleanup=True)\n",
        "        \n",
        "        self.spectrogram_width = self.__get_image_width(self.images_dir_name) #get the width of each spectrogram\n",
        "        \n",
        "        if no_saved_labels == False:\n",
        "            if create_labels == True:\n",
        "            #create and save new json files that represent the labels, overriding the old ones in self.labels_dir_name\n",
        "                for i in range(0,len(self.filenums),1):\n",
        "                    #save_json_label_for_song(song_filename, save_directory, intervaltree_obj, width, interval, sr=None)\n",
        "                    save_json_label_for_song(self.unprocessed_dataset[self.filenums[i]][0], self.labels_dir_name, self.unprocessed_dataset[self.filenums[i]][1], width=self.spectrogram_width, interval=self.interval)\n",
        "\n",
        "            list_of_label_paths = []\n",
        "            #get a list of the filenames for the labels (json files)\n",
        "            for entry in os.scandir(self.labels_dir_name):\n",
        "                if entry.path.endswith(\".json\") and entry.is_file():\n",
        "                    list_of_label_paths += [entry.path]\n",
        "\n",
        "            for i in range(0,len(self.filenums),1):\n",
        "                if self.filenums[i] == int(os.path.basename(list_of_label_paths[i])[0:-5]):\n",
        "                    song_labels_dict = JsonToDictWithNumpy(list_of_label_paths[i])\n",
        "                    for key in song_labels_dict: #note that key is a string\n",
        "                        start_and_end_time = key.split('_')\n",
        "                        current_sample = dict()\n",
        "                        current_sample['filenum'] = self.filenums[i]\n",
        "                        current_sample['start_time'] = float(start_and_end_time[1])\n",
        "                        current_sample['end_time'] = float(start_and_end_time[2])\n",
        "                        current_sample['spectrogram'] = cv.imread(os.path.join(self.images_dir_name,key)+'.jpg', cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "                        current_sample['transcription'] = song_labels_dict[key]\n",
        "                        self.processed_data_list.append(current_sample)\n",
        "                else:\n",
        "                    raise Exception(\"ERROR: Missing file or label number \" + str(self.filenums[i]))\n",
        "        else:\n",
        "            for i in range(0,len(self.filenums),1):\n",
        "                song_labels_dict = get_label_dict_for_song(self.unprocessed_dataset[self.filenums[i]][0], self.unprocessed_dataset[self.filenums[i]][1], width=self.spectrogram_width, interval=self.interval)\n",
        "                for key in song_labels_dict: #note that key is a string\n",
        "                    start_and_end_time = key.split('_')\n",
        "                    current_sample = dict()\n",
        "                    current_sample['filenum'] = self.filenums[i]\n",
        "                    current_sample['start_time'] = float(start_and_end_time[1])\n",
        "                    current_sample['end_time'] = float(start_and_end_time[2])\n",
        "                    current_sample['spectrogram'] = cv.imread(os.path.join(self.images_dir_name,key)+'.jpg', cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "                    current_sample['transcription'] = song_labels_dict[key]\n",
        "                    self.processed_data_list.append(current_sample)\n",
        "        \n",
        "        self.processed_dataset = pd.DataFrame(data=self.processed_data_list, columns=['filenum','start_time','end_time','spectrogram','transcription'])\n",
        "\n",
        "    ######################################################################################################################## \n",
        "    # Built-in functions in torch.utils.data.Dataset that must be overridden accordingly\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.processed_dataset.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_dataset.iloc[idx,:].to_dict()\n",
        "        \n",
        "    ########################################################################################################################   \n",
        "    \n",
        "    \n",
        "    def __get_image_width(self, directory):\n",
        "        \"\"\"\n",
        "        Private method. Gets the width of the first image in directory.\n",
        "        \"\"\"\n",
        "        image_filename = ''\n",
        "        list_of_files = []\n",
        "        for entry in os.scandir(self.images_dir_name):\n",
        "            if entry.path.endswith(\".jpg\") and entry.is_file():\n",
        "                list_of_files += [entry.path]\n",
        "\n",
        "        img = cv.imread(list_of_files[0], -1)\n",
        "        return img.shape[0]\n",
        "\n",
        "    def visualize_sample(self, idx):\n",
        "        \"\"\"\n",
        "        Call this function to visualize the idx-th sample in this dataset\n",
        "        \"\"\"\n",
        "        sample = self.__getitem__(idx)\n",
        "        \n",
        "        display_dict = dict()\n",
        "        display_dict['filenum'] = [sample['filenum']]\n",
        "        display_dict['start_time'] = [sample['start_time']]\n",
        "        display_dict['end_time'] = [sample['end_time']]\n",
        "        display_df = pd.DataFrame.from_dict(display_dict)\n",
        "        print(\"\\nInformation about sample number \" + str(idx) + \":\\n\", display_df)\n",
        "\n",
        "        print(\"\\nInput and Label:\")\n",
        "        fig = plt.figure(figsize=(18,6), dpi=57)\n",
        "        plt.imshow(sample['spectrogram'], cmap='gray')\n",
        "        plot_label_3D_array(sample['transcription'])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V12HvZe9CbN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Ft1srROzOt"
      },
      "source": [
        "Create an instance of the `MusicNetData` dataloader class to get the dataset. Note that the `MusicNetData` class inherits from `torch.utils.data.Dataset`. If you are running the code for the first time, set `create_spec=True` when initializing an instance of `MusicNetData`. This will create and save spectrograms of each song.<br><br>\n",
        "Suppose the variable `dataset` is an instance of `MusicNetData`, that is, `dataset = MusicNetData(...)`. Every sample in `dataset` is a dictionary representing an interval of a song. This dictionary has keys: filenum, start_time, end_time, spectrogram, and transcription. The `i`th sample can be accessed via `dataset[i]`. The length of the dataset can be found by calling `len(dataset)`. The `i`th sample can be visualized by calling `dataset.visualize_sample(i)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301BQwZYKrjr"
      },
      "source": [
        "train_set = MusicNetDataset(dataset_type='train', create_spec=False) #loading the data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "\n",
        "print(train_set[100]) #see what the 0th sample looks like\n",
        "print(len(train_set)) #length of train_set\n",
        "#make sure inputs are 0-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqx1mz8VyiFQ"
      },
      "source": [
        "print(train_set.processed_dataset.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvtYXqQ4yiFU"
      },
      "source": [
        "test_set = MusicNetDataset(dataset_type='test', create_spec=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV_pLn0-yiFX"
      },
      "source": [
        "def get_instrument_onehot(transcription):\n",
        "    return (transcription.sum(dim=(1,2)) != 0).long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "clAjdRx9yiFh"
      },
      "source": [
        "print(train_set[0]['spectrogram'].dtype)\n",
        "\n",
        "for sample in train_loader:\n",
        "    print(get_instrument_onehot(sample['transcription']).shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzHvF7vRPADb",
        "scrolled": false
      },
      "source": [
        "train_set.visualize_sample(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9vfSY6DN4lC"
      },
      "source": [
        "Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-JdXSNxyiFr"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY-PsOawyiFw"
      },
      "source": [
        "#spectrogram resolution: 453x1395\n",
        "\n",
        "\n",
        "class Baseline_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Baseline_Net, self).__init__()\n",
        "        self.name = \"baseline\"\n",
        "        self.layer1 = nn.Linear(453*1395, 100)\n",
        "        self.layer2 = nn.Linear(100,1)\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 453*1395)\n",
        "        activation1 = self.layer1(flattened)\n",
        "        activation1 = F.relu(activation1)\n",
        "        activation2 = self.layer2(activation1)\n",
        "        return activation2.squeeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxKTkhUDOFsv"
      },
      "source": [
        "\n",
        "class Primary_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Primary_Net, self).__init__()\n",
        "        self.name = \"primary\"\n",
        "        self.conv1 = nn.Conv2d(1, 5, 5)\n",
        "        self.batch1 = nn.BatchNorm2d(5)   \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
        "        self.batch2 = nn.BatchNorm2d(10) \n",
        "        self.conv3 = nn.Conv2d(10, 5, 5)\n",
        "        self.batch3 = nn.BatchNorm2d(5) \n",
        "        self.fc1 = nn.Linear(53 * 170 * 5, 32) #height x width x number_of_channels\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv3(x)))\n",
        "        x = x.view(-1, 53 * 170 * 5)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x\n",
        "    \n",
        "#add more layers\n",
        "#change conv2d to 1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBc4KPGWyiF4"
      },
      "source": [
        "baseline_net = Baseline_Net()\n",
        "\n",
        "for param in baseline_net.parameters():\n",
        "    print (\"Baseline_Net\", param.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ud4blBCyiF-"
      },
      "source": [
        "def plot_training_curve(path):\n",
        "    \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    n = len(train_err) # number of epochs\n",
        "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ofB5RYyiGB"
      },
      "source": [
        "def get_model_name(instrument, name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"instrument_{0}_model{1}_bs{2}_lr{3}_epoch{4}\".format(instrument, name, batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path\n",
        "\n",
        "def evaluate(instrument, net, loader, criterion):\n",
        "    \"\"\" Evaluate the network on the validation set.\n",
        "\n",
        "     Args:\n",
        "         net: PyTorch neural network object\n",
        "         loader: PyTorch data loader for the validation set\n",
        "         criterion: The loss function\n",
        "     Returns:\n",
        "         err: A scalar for the avg classification error over the validation set\n",
        "         loss: A scalar for the average loss function over the validation set\n",
        "     \"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_epoch = 0\n",
        "    for i,data in enumerate(loader,0):\n",
        "        #inputs = torch.from_numpy(data['spectrogram']).unsqueeze(0).unsqueeze(0)\n",
        "        inputs = data['spectrogram']\n",
        "        labels = get_instrument_onehot(data['transcription'])[:,instrument]\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        corr = (outputs > 0.0).squeeze().long() != labels\n",
        "        total_err += int(corr.sum())\n",
        "        total_loss += loss.item()\n",
        "        total_epoch += len(labels)\n",
        "    err = float(total_err) / total_epoch\n",
        "    loss = float(total_loss) / (i + 1)\n",
        "    return err, loss\n",
        "\n",
        "\n",
        "def train_single_instrument(instrument, net, batch_size = 16, learning_rate=0.01, num_epochs=30):\n",
        "    ########################################################################\n",
        "    \n",
        "    ########################################################################\n",
        "    # Fixed PyTorch random seed for reproducible result\n",
        "    torch.manual_seed(1000)\n",
        "    ########################################################################\n",
        "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
        "    #train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "    #        target_classes, batch_size)\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # The loss function will be Binary Cross Entropy (BCE). In this case we\n",
        "    # will use the BCEWithLogitsLoss which takes unnormalized output from\n",
        "    # the neural network and scalar label.\n",
        "    # Optimizer will be SGD with Momentum.\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    #change momentum to 0.5-1.5\n",
        "    #change optimizer: Adam\n",
        "    ########################################################################\n",
        "    # Set up some numpy arrays to store the training/test loss/erruracy\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    # Loop over the data iterator and sample a new batch of training data\n",
        "    # Get the output from the network, and optimize our loss function.\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        for i,data in enumerate(train_loader,0):\n",
        "            # Get the inputs\n",
        "            inputs = data['spectrogram']\n",
        "            labels = get_instrument_onehot(data['transcription'])[:,instrument]\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            #print(loss)\n",
        "            #print(labels)\n",
        "            #print(outputs)\n",
        "            #print(net.layer1.weight.grad.norm())\n",
        "            optimizer.step()\n",
        "        \n",
        "            # Calculate the statistics\n",
        "            corr = (outputs > 0.0).squeeze().long() != labels\n",
        "            total_train_err += int(corr.sum())\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(instrument, net, test_loader, criterion)\n",
        "        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n",
        "               \"Validation err: {}, Validation loss: {}\").format(\n",
        "                   epoch + 1,\n",
        "                   train_err[epoch],\n",
        "                   train_loss[epoch],\n",
        "                   val_err[epoch],\n",
        "                   val_loss[epoch]))\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(instrument, net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "    # Write the train/test loss/err into CSV file for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
        "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
        "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
        "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
        "    plot_training_curve(model_path)\n",
        "    return model_path\n",
        "\n",
        "def train_nets(nets, batch_size=16, learning_rate=0.01, num_epochs=30):\n",
        "    array = []\n",
        "    for i, net in enumerate(nets):\n",
        "        print(\"training instrument: \", i)\n",
        "        train_single_instrument(i, net, batch_size, learning_rate, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6BSQtmNyiGF"
      },
      "source": [
        "testnet1 = Baseline_Net()\n",
        "nets = [Baseline_Net() for i in range(11)]\n",
        "train_nets(nets, num_epochs=20)\n",
        "#change learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E0JtFw8yiGI"
      },
      "source": [
        "model_path = get_model_name(10,\"baseline\", batch_size=32,learning_rate=0.01, epoch=9)\n",
        "plot_training_curve(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fpOfbPsyiGL"
      },
      "source": [
        "testnet2 = Primary_Net()\n",
        "nets = [Primary_Net() for i in range(11)]\n",
        "train_nets(nets, batch_size = 128, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JifhbohNyiGP"
      },
      "source": [
        "def get_instrument_model(num):\n",
        "    return \"instrument_{}_modelprimary_lr0.01_epoch9\".format(num)\n",
        "\n",
        "nets = [Primary_Net() for i in range(11)]\n",
        "\n",
        "for i, net in enumerate(nets):\n",
        "    net.load_state_dict(torch.load(get_instrument_model(i)))\n",
        "    net.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoZOKnflyiGT"
      },
      "source": [
        "image = Image.open(\"/Users/katherineliang/PycharmProjects/aps360/musicnet/holdout_images/1742_0770_0780.jpg\")\n",
        "display(image)\n",
        "\n",
        "spectrogram = ToTensor()(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHF30rLwyiGW"
      },
      "source": [
        "names = ['piano', 'violin', 'viola', 'cello', 'clarinet', 'bassoon', 'horn', 'oboe', 'flute', 'harpsichord', 'string bass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMZpjWC7yiGY"
      },
      "source": [
        "for i, net in enumerate(nets):\n",
        "    result = net(spectrogram).item()\n",
        "    print(names[i], result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WkZ5OF4yiGa"
      },
      "source": [
        "nets = [Primary_Net() for i in range(128)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXfmCwf7yiGd"
      },
      "source": [
        "notes = []\n",
        "\n",
        "for i, net in enumerate(nets):\n",
        "    result = net(spectrogram).item()\n",
        "    if result>0:\n",
        "        notes.append(i)\n",
        "print(notes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79lLHFE2yiGf"
      },
      "source": [
        "image = Image.open(\"/Users/katherineliang/PycharmProjects/aps360/musicnet/holdout_images/1742_0770_0780_56.jpg\")\n",
        "display(image)\n",
        "\n",
        "spectrogram = ToTensor()(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPGzLm9yiGi"
      },
      "source": [
        "notes = []\n",
        "\n",
        "for i, net in enumerate(nets):\n",
        "    result = net(spectrogram).item()\n",
        "    if result>0:\n",
        "        notes.append(i)\n",
        "print(notes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}